components:
  terraform:
    eks/cluster/disabled:
      metadata:
        component: eks/cluster
      vars:
        enabled: false

        # Cluster node configuration
        aws_ssm_agent_enabled: true
        managed_node_groups_enabled: true
        node_groups: # will create node group for each item in map
          main: # Karpenter is responsible for scaling nodes, but this default node group is required for deploying EKS Addons
            # EKS AMI version to use, e.g. "1.16.13-20200821" (no "v").
            ami_release_version: null
            # Type of Amazon Machine Image (AMI) associated with the EKS Node Group
            ami_type: AL2_x86_64
            # Additional name attributes (e.g. `1`) for the node group
            attributes: []
            # will create 1 auto scaling group in each specified availability zone
            # or all AZs with subnets if none are specified anywhere
            availability_zones: null
            # Whether to enable Node Group to scale its AutoScaling Group
            cluster_autoscaler_enabled: false
            # True (recommended) to create new node_groups before deleting old ones, avoiding a temporary outage
            create_before_destroy: true
            # Desired number of worker nodes when initially provisioned
            desired_group_size: 1
            # Enable disk encryption for the created launch template (if we aren't provided with an existing launch template)
            disk_encryption_enabled: true
            # Disk size in GiB for worker nodes. Terraform will only perform drift detection if a configuration value is provided.
            disk_size: 20
            # Set of instance types associated with the EKS Node Group. Terraform will only perform drift detection if a configuration value is provided.
            instance_types:
              - t3.small
            kubernetes_labels: {}
            kubernetes_taints: []
            node_role_policy_arns: null
            kubernetes_version: null
            max_group_size: 3
            min_group_size: 1
            resources_to_tag:
              - instance
              - volume
            tags: null

        access_config:
          authentication_mode: "API"
          bootstrap_cluster_creator_admin_permissions: true

        # Fargate profiles, required for Karpenter
        fargate_profiles:
          karpenter:
            kubernetes_namespace: karpenter
            kubernetes_labels: null
        fargate_profile_iam_role_kubernetes_namespace_delimiter: "@"
        karpenter_iam_role_enabled: true

        # Legacy settings
        # The upstream component sets these to true by default to avoid breaking existing deployments,
        # but new deployments should have these settings all disabled.
        legacy_fargate_1_role_per_profile_enabled: false

        addons_depends_on: true
        deploy_addons_to_fargate: false

        # Network access configuration
        allow_ingress_from_vpc_accounts: []
          # - tenant: core
          #   stage: auto
          # - tenant: core
          #   stage: network
        public_access_cidrs: ["0.0.0.0/0"]
        allowed_cidr_blocks: []
        allowed_security_groups: []

        enabled_cluster_log_types: []
          # Caution: enabling `api` log events may lead to a substantial increase in Cloudwatch Logs expenses.
          # - api
          # - audit
          # - authenticator
          # - controllerManager
          # - scheduler

        # EKS IAM Authentication settings
        # By default, you can authenticate to EKS cluster only by assuming the role that created the cluster.
        # After the Auth Config Map is applied, the other IAM roles in
        # `aws_teams_rbac`, `aws_team_roles_rbac`, and `aws_sso_permission_sets_rbac` will be able to authenticate.
        apply_config_map_aws_auth: true
        availability_zone_abbreviation_type: fixed
        cluster_private_subnets_only: true
        cluster_encryption_config_enabled: true
        cluster_endpoint_private_access: true
        cluster_endpoint_public_access: true
        cluster_log_retention_period: 90
        oidc_provider_enabled: true

        # List of `aws-teams` to map to Kubernetes RBAC groups.
        # This gives teams direct access to Kubernetes without having to assume a team-role.
        # RBAC groups must be created elsewhere. The "system:" groups are predefined by Kubernetes.
        aws_teams_rbac: []
          # - aws_team: managers
          #   groups:
          #     - system:masters
          # - aws_team: devops
          #   groups:
          #     - system:masters

        # List of `aws-teams-roles` (in the account where the EKS cluster is deployed) to map to Kubernetes RBAC groups
        aws_team_roles_rbac: []
          # - aws_team_role: admin
          #   groups:
          #     - system:masters
          # - aws_team_role: poweruser
          #   groups:
          #     - idp:poweruser
          # - aws_team_role: observer
          #   groups:
          #     - idp:observer
          # - aws_team_role: planner
          #   groups:
          #     - idp:reader
          # - aws_team_role: terraform
          #   groups:
          #     - system:masters

        # Permission sets from AWS SSO allowing cluster access
        # See `aws-sso` component.
        aws_sso_permission_sets_rbac: []
        # - aws_sso_permission_set: PowerUserAccess
        #   groups:
        #   - idp:poweruser
        # - aws_sso_permission_set: ReadOnlyAccess
        #   groups:
        #   - idp:observer

        cluster_kubernetes_version: "1.30"

        # You can set all the add-on versions to `null` to use the latest version,
        # but that introduces drift as new versions are released. As usual, we recommend
        # pinning the versions to a specific version and upgrading when convenient.

        # Determine the latest version of the EKS add-ons for the specified Kubernetes version
        # Maybe you can do it with custom command `atmos eks addons`. Otherwise:
        #
        #  EKS_K8S_VERSION=1.30 # replace with your cluster version
        #  ADD_ON=vpc-cni # replace with the add-on name
        #  echo "${ADD_ON}:" && aws eks describe-addon-versions --kubernetes-version $EKS_K8S_VERSION --addon-name $ADD_ON \
        #  --query 'addons[].addonVersions[].{Version: addonVersion, Defaultversion: compatibilities[0].defaultVersion}' --output table

        # To see versions for all the add-ons, wrap the above command in a for loop:
        #   for ADD_ON in vpc-cni kube-proxy coredns aws-ebs-csi-driver aws-efs-csi-driver; do
        #     echo "${ADD_ON}:" && aws eks describe-addon-versions --kubernetes-version $EKS_K8S_VERSION --addon-name $ADD_ON \
        #     --query 'addons[].addonVersions[].{Version: addonVersion, Defaultversion: compatibilities[0].defaultVersion}' --output table
        #   done

        # https://docs.aws.amazon.com/eks/latest/userguide/eks-add-ons.html
        # https://docs.aws.amazon.com/eks/latest/userguide/managing-add-ons.html#creating-an-add-on
        # https://marcincuber.medium.com/amazon-eks-upgrade-journey-from-1-29-to-1-30-say-hello-to-cute-uwubernetes-eba082199cc4
        # https://docs.aws.amazon.com/eks/latest/userguide/workloads-add-ons-available-eks.html
        addons:
          # Get latest version of the addons with the `atmos eks addons eks/cluster -s kma-ue2-staging` command
          #
          # https://docs.aws.amazon.com/eks/latest/userguide/cni-iam-role.html
          # https://docs.aws.amazon.com/eks/latest/userguide/managing-vpc-cni.html
          # https://docs.aws.amazon.com/eks/latest/userguide/cni-iam-role.html#cni-iam-role-create-role
          # https://aws.github.io/aws-eks-best-practices/networking/vpc-cni/#deploy-vpc-cni-managed-add-on
          vpc-cni:
            addon_version: "v1.18.3-eksbuild.3" # set `addon_version` to `null` to use the latest version
            # Leave resolve_conflicts_* at default value, which is "OVERWRITE" for both
            #  resolve_conflicts_on_create: "OVERWRITE"
            #  resolve_conflicts_on_update: "OVERWRITE"
          # https://docs.aws.amazon.com/eks/latest/userguide/managing-kube-proxy.html
          kube-proxy:
            addon_version: "v1.30.3-eksbuild.5" # set `addon_version` to `null` to use the latest version
          # https://docs.aws.amazon.com/eks/latest/userguide/managing-coredns.html
          coredns:
            addon_version: "v1.11.3-eksbuild.1" # set `addon_version` to `null` to use the latest version
            ## override default replica count of 2
            configuration_values: '{"autoScaling":{"enabled":true,"minReplicas":3}}'
          # https://docs.aws.amazon.com/eks/latest/userguide/csi-iam-role.html
          # https://aws.amazon.com/blogs/containers/amazon-ebs-csi-driver-is-now-generally-available-in-amazon-eks-add-ons
          # https://docs.aws.amazon.com/eks/latest/userguide/managing-ebs-csi.html#csi-iam-role
          # https://github.com/kubernetes-sigs/aws-ebs-csi-driver
          aws-ebs-csi-driver:
            addon_version: "v1.34.0-eksbuild.1" # set `addon_version` to `null` to use the latest version
            # If you are not using [volume snapshots](https://kubernetes.io/blog/2020/12/10/kubernetes-1.20-volume-snapshot-moves-to-ga/#how-to-use-volume-snapshots)
            # (and you probably are not), disable the EBS Snapshotter with:
            configuration_values: '{"sidecars":{"snapshotter":{"forceEnable":false}}}'
          # https://www.eksworkshop.com/docs/fundamentals/storage/efs/efs-csi-driver
          # https://github.com/kubernetes-sigs/aws-efs-csi-driver
          aws-efs-csi-driver:
            addon_version: "v2.0.8-eksbuild.1"